我们在这里将要学习 MCP 协议中最强大的功能之一：**Sampling**（采样）。

这里 Sampling 的意思是，MCP Server 将部分工作委托回 MCP Client 端的 LLM 来实现。比如，我们想实现一个获取文本摘要的 Tool，输入是文件名，但是编程我们只能通过文件名获取文件内容，不能做到摘要。因此，可以将这个部分内容委托为客户端的 LLM 来实现。

## Motivation

LLM sampling 创建了一个倒置或双向的架构，通常，是 LLM 客户端调用服务器的工具，但使用采样时，服务器也可以回调客户端的模型。对于需要执行语言理解或生成相关功能的 Tool 来说，常常需要 AI 驱动，如果服务器自行集成 LLM API 或在本地运行模型，这会导致成本高昂且难以扩展。而将对应的工作卸载到客户端的模型执行提供了几个优势：

- **Scalability**：避免了服务器进行大量的推理，意味着可以处理更多的并发请求；
- **Cost efficiency**：将成本分摊到用户，任何与 LLM 相关的 API 成本或计算负载均由客户承担；
- **Flexibility in LLM choice**：客户端可以选择用于处理请求的模型，服务器最多仅提供一个模型偏好。

MCP 中的 Sampling 是连接确定性的服务器逻辑和动态的文本生成之间的桥梁，只需通过标准化的协议调用就可以实现。

## Architecture

在正常的交互中，客户端发送请求，服务器执行必要的操作并将结果返回给客户端。采样引入了一种反向交互：当服务器处理请求时，可以要求客户端执行一项 AI 任务。

![[Pasted image 20250624174958.png]]

FastMCP 的客户端库显式地提供一个采样处理程序（回调函数）来帮助我们处理这些请求。

## Transport

MCP 协议确保这些请求能够可靠地传输。FastMCP 支持多种传输方式（如 `stdio` 和 `sse` ）。

无论使用何种传输方式，流程都是一样的。服务器在 `Context` 类（如 `ctx.sample(…)` ）的实例/对象上调用采样方法，将其打包成 MCP 消息，发送给客户端，并等待响应。

客户端调用采样处理程序，该处理程序又调用实际的 LLM，并返回完成结果。所有这些操作都是异步的，也就是说，服务器的工具协程将挂起，直到结果返回，从而避免阻塞其他任务。

抽样请求只是另一种结构化请求类型，因此所有常规的 MCP 验证都适用。

## Context

要在 FastMCP 中使用采样，服务器会向其函数提供一个 `Context` 对象。

`Context` 是一个功能强大的句柄，除了请求 LLM 采样外，还可以让服务器代码访问日志、发送更新等。

FastMCP 在将此上下文作为参数包含时会自动将其注入工具函数中。
