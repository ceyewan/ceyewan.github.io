---
date: 2025-09-23T12:03:26+08:00
draft: true
title: 未命名
slug: 20250923-q5qa0q7q
tags:
  - 标签
categories:
  - 分类
---

## 1 vLLM

### 1.1 核心概念

**持续批处理 (Continuous Batching)**：vLLM 的核心调度模型。它在每个推理时间步（iteration）动态地将新请求加入运行中的批次，并移除已完成的请求。这打破了传统静态批处理必须等待整个批次完成的限制，通过消除 GPU 空闲和 padding 计算，将 GPU 利用率和吞吐量最大化。

### 1.2 PagedAttention

**PagedAttention** 是实现持续批处理的底层内存管理技术。它借鉴操作系统虚拟内存的分页机制，将物理上不连续的 GPU 显存块（Blocks）映射到逻辑上连续的 KV Cache 序列。

-   **核心优势**：
    1.  **消除内存碎片**：通过管理固定大小的 Block，避免了因动态分配导致的外部碎片。
    2.  **按需分配**：KV Cache 按需分配 Block，几乎消除了预分配带来的内存浪费。
    3.  **高效共享**：通过复制块表（Block Table）实现写时复制（Copy-on-Write），极大优化了 Beam Search 和并行采样等场景的内存占用。

### 1.3 关注和优化点

作为调度工程师，需在 vLLM 基础上进行上层策略优化：

1.  **调度策略 (Scheduling Policy)**：设计超越默认 FIFO 的策略，如基于优先级的抢占式调度，以满足不同用户的 SLA。
2.  **抢占与换出 (Preemption & Swapping)**：为高优请求，可将低优请求的 KV Cache Blocks 换出（Swap Out）到 CPU 内存或 NVMe，待资源可用时再换入（Swap In）恢复执行。
3.  **接纳控制 (Admission Control)**：设计智能策略，在系统高负载时决策是否接纳新请求，以防止 OOM 并平衡吞吐量与延迟。
4.  **指标与权衡 (Metrics & Trade-offs)**：在吞吐量 (Throughput)、首字延迟 (TTFT)、每 Token 延迟 (TPOT) 和 GPU 利用率 (MFU) 之间进行权衡与优化。

### 1.4 单机与千卡集群

-   **单机实例**：一个 vLLM 实例（Engine）通常管理一个物理节点内的 1-8 个 GPU，利用 NVLink/NVSwitch 进行高效的**张量并行 (Tensor Parallelism)**。
-   **千卡集群**：通过部署多个独立的 vLLM 实例来构建。上层需要一个**API Gateway/Inference Router** 作为流量入口，负责**模型路由**（根据请求模型分发到对应实例集群）和**负载均衡**（在同一模型的多个副本实例间分发流量）。

### 1.5 会话粘性

LLM 推理是有状态的，KV Cache 构成了会话状态。在多实例集群中，必须确保一个会话的所有连续请求都被路由到持有其 KV Cache 的同一个 vLLM 实例。

-   **实现方式**：
    1.  **粘性会话 (Sticky Sessions)**：API Gateway 为每个新会话生成唯一 ID，并在外部状态存储（如 Redis）中记录 `Session ID -> Instance IP` 的映射。后续请求凭 Session ID 被路由到固定实例。
    2.  **状态解耦**：更高级的架构中，将 KV Cache 换出到分布式缓存，允许请求在不同实例间迁移，提升容错性和负载均衡能力，但会引入额外延迟。

### 1.6 传输协议

-   **客户端到 Gateway (外部)**：主流采用 **Server-Sent Events (SSE)**。它基于 HTTP，标准、简单且完美匹配 LLM 单向流式输出的场景，是 OpenAI API 的事实标准。
-   **Gateway 到 vLLM (内部)**：
    1.  **HTTP/SSE 透传**：架构简单，易于实现和调试。
    2.  **gRPC Streaming**：性能更优。Gateway 接收外部 SSE，转换为高性能的 gRPC 流调用内部 vLLM 服务，vLLM 需实现 gRPC Server。此模式在大型系统中更受欢迎。

### 1.7 k8s 千卡集群

在 Kubernetes 上部署大规模 vLLM 服务是标准实践。

-   **GPU 资源化**：通过 **NVIDIA Device Plugin** 使 K8s 能够调度 `nvidia.com/gpu` 资源。
-   **部署**：每个 vLLM 模型版本作为一个 `Deployment` 进行部署，每个 Pod 请求所需的多 GPU 资源。模型权重通常通过挂载共享存储（如 NAS/CPFS 的 PVC）来加载。
-   **服务暴露**：通过 `Service` 为一组 vLLM Pod 提供稳定的内部端点。API Gateway 通过 `Ingress` 或 `LoadBalancer` Service 对外暴露。
-   **弹性伸缩**：利用 **HPA** (Horizontal Pod Autoscaler) 根据负载自动增减 Pod 副本，结合 **Cluster Autoscaler** 自动增减 GPU 节点，实现成本优化。

### 1.8 组调度

默认 K8s 调度器逐 Pod 调度，在调度多 GPU Pod 时易引发资源碎片和死锁。**组调度 (Gang Scheduling)** 通过 **"All-or-Nothing"** 策略解决此问题。

-   **实现方案**：
    1.  **Volcano**：引入 `PodGroup` CRD，原子化地调度一组 Pod。
    2.  **Kueue (Queueing for Kubernetes)**：提供作业排队系统，通过暂停/恢复机制与默认调度器协同，实现组调度，并提供配额、优先级和抢占等高级功能。

### 1.9 流量管理

在云原生环境中，流量管理主要由 **Service Mesh (如 Istio/ASM)** 或 **API Gateway** 实现。

-   **核心能力**：
    -   **动态路由**：实现金丝雀发布、蓝绿部署。
    -   **负载均衡**：提供多种负载均衡策略。
    -   **流量策略**：配置超时 (Timeouts)、重试 (Retries)、熔断 (Circuit Breaking) 来提升系统韧性。
    -   **安全**：通过 mTLS 实现服务间通信加密。

### 1.10 服务治理

服务治理是流量管理的超集，涵盖了微服务的整个生命周期管理。

-   **关键领域**：
    -   **服务发现**：Kubernetes Service 提供了基础的服务发现能力。
    -   **配置管理**：使用 `ConfigMap` 和 `Secrets` 管理应用配置和敏感信息。
    -   **网络策略**：通过 `NetworkPolicy` 实现 Pod 间的网络隔离和访问控制。
    -   **多租户**：利用 `Namespace`、`ResourceQuota` 和 `RBAC` 实现资源和权限的隔离。

### 1.11 可观测性

构建全面的可观测性体系是保障服务稳定运行的前提。

-   **Metrics (指标)**：
    -   **业务指标**：QPS、错误率、端到端延迟。
    -   **性能指标**：TTFT, TPOT, Token 吞吐量, KV Cache 利用率。
    -   **硬件指标**：GPU/显存利用率、功耗、温度。
    -   **技术栈**：Prometheus + Grafana + DCGM Exporter。
-   **Logging (日志)**：
    -   **内容**：API Gateway 访问日志、vLLM 调度决策日志、Worker 错误日志。
    -   **实践**：采用结构化日志 (JSON)，统一收集到 SLS/Loki/Elasticsearch。
-   **Tracing (追踪)**：
    -   **价值**：通过 OpenTelemetry 将请求在分布式系统中的完整调用链可视化，精准定位性能瓶颈。
    -   **技术栈**：OpenTelemetry + Jaeger/SkyWalking。

## 2 Ws 万级连接

1. **异步非阻塞 I/O**：Go 的 `net` 包底层基于 epoll (Linux) / kqueue (BSD) / IOCP (Windows)，天然就是异步非阻塞的，这是我们能处理高并发的基础。
2. **协程 (Goroutine) 模型**：为每个连接分配一个独立的 Goroutine 来处理读写，是 Go 最自然、最符合直觉的模型。`1 连接 = 1 读 Goroutine + 1 写 Goroutine` 是一个常见的模式。
3. **资源复用**：高并发下，频繁的对象创建和销毁是性能杀手。内存池（特别是对 Buffer）和对象池的使用至关重要。
4. **内核参数优化**：文件描述符、TCP/IP 协议栈调优、负载均衡水平扩展

## 3 熔断、限流、重试与负载均衡

### 3.1 熔断 (Circuit Breaker)

**目的：** 保护调用方，防止因下游服务故障导致雪崩效应。
**原理：** 模拟电路保险丝。通过状态机（Closed -> Open -> Half-Open）监控下游服务调用成功率/失败率。当失败率达到阈值时，断路器打开（Open），阻止新请求流向下游，快速失败或执行降级逻辑。一段时间后进入半开（Half-Open）状态，允许少量试探性请求，若成功则恢复闭合（Closed）。
**实现：**
-   **Go：** `github.com/sony/gobreaker`。
-   **集成：** 作为 gRPC **客户端拦截器**，包裹对下游服务的实际调用。
-   **降级：** 当断路器打开时，捕获 `gobreaker.ErrOpenState` 错误，执行预设的降级逻辑（如返回缓存、默认值）。
**与慢响应：** 配合客户端超时（`context.WithTimeout`），将慢响应转化为失败，触发熔断。
**粒度：** 理想情况下，熔断器应针对**每个下游服务实例**独立维护，与负载均衡器协同工作。

### 3.2 限流 (Rate Limiting)

**目的：** 保护被调用方，防止因流量过载导致服务崩溃。
**原理：** 令牌桶算法。系统以固定速率向桶中放入令牌，请求需获取令牌才能通过。桶有容量限制，溢出令牌丢弃。允许短时突发流量，同时控制长期平均速率。
**实现：**
-   **分布式：** 基于 Redis + Lua 脚本。Redis 存储全局令牌桶状态，Lua 脚本保证原子性。
    -   **位置：** gRPC **服务端拦截器**。作为第一道防线，快速拒绝超限请求，保护服务端资源。
    -   **优点：** 全局一致性，精确控制总流量。
    -   **缺点：** 引入 Redis 依赖，网络开销。
-   **单机：** Go 标准库 `golang.org/x/time/rate`。
    -   **位置：** gRPC **客户端拦截器**。
    -   **优点：** 内存操作，性能高，无外部依赖，减轻下游压力。
    -   **缺点：** 仅限当前进程，无法控制全局流量。
**推荐策略：** 服务端分布式限流（强制）+ 客户端单机限流（"好公民"行为）。

### 3.3 重试 (Retry)

**目的：** 提高请求成功率，处理瞬时错误（网络抖动、临时过载等）。
**原理：** 在请求失败后，根据预设策略（最大重试次数、指数退避延迟、可重试错误码）重新发送请求。
**实现：**
-   **gRPC：** 客户端通过 `grpc.WithDefaultServiceConfig` 配置重试策略（JSON），指定 `MaxAttempts`、`InitialBackoff`、`RetryableStatusCodes`。
-   **Kafka 生产者：** 配置 `sarama.Config.Producer.Retry.Max` 和 `sarama.Config.Producer.Idempotent=true`（开启幂等性）。
-   **Kafka 消费者：** 通常在业务代码内部实现本地重试，或结合死信队列 (DLQ)、延迟重试主题等模式。
**关键：** **幂等性**。只有幂等操作才适合重试，避免副作用。

### 3.4 负载均衡 (Load Balancing)

**目的：** 将请求分发到多个后端服务实例，优化资源利用，提高吞吐量和可用性。
**类型：**
-   **服务端负载均衡 (Server-Side LB)：** Nginx, F5, ELB。
    -   **原理：** 独立中间层代理，客户端请求先到 LB，LB 再转发到后端。
    -   **优点：** 对客户端透明，集中管理，功能丰富。
    -   **缺点：** 单点风险，额外网络跳数，性能瓶颈。
-   **客户端负载均衡 (Client-Side LB)：** gRPC 客户端内置 LB，Spring Cloud Ribbon。
    -   **原理：** 客户端通过服务发现获取后端列表，内部实现 LB 算法，直接连接后端。
    -   **优点：** 无单点，减少网络跳数，更细粒度控制，与客户端治理紧密集成。
    -   **缺点：** 客户端复杂性增加，多语言支持挑战。
**与熔断配合：** 客户端负载均衡器应感知熔断器状态。当某个后端实例被熔断时，负载均衡器应将其从可用池中移除，将请求路由到其他健康实例，实现"半断"而非"全断"。

### 3.5 拦截器/中间件的合理顺序

**核心原则：** 越早拒绝越好，保护自身优先，重试在最后且考虑幂等性。

**gRPC 客户端拦截器 (从外到内)：**
1.  **客户端本地限流：** 保护客户端自身，快速拒绝超速请求。
2.  **客户端分布式限流 (可选)：** 限制所有客户端实例对下游的总流量。
3.  **客户端熔断：** 隔离故障下游，快速失败或降级。
4.  **负载均衡：** 选择健康的后端实例（应避开熔断实例）。
5.  **gRPC 客户端内置重试：** 处理瞬时错误，提高成功率（仅限幂等操作）。
6.  **实际 gRPC 调用**

**gRPC 服务端拦截器 (从外到内)：**
1.  **服务端限流：** 保护服务端自身，快速拒绝过载请求。
2.  **认证/授权：** 验证请求合法性。
3.  **日志/追踪：** 记录请求信息。
4.  **实际业务逻辑**

**Service Mesh (服务网格)：**
-   **模式：** 平台模式，通过 Sidecar 代理（如 Envoy）将熔断、限流、负载均衡、重试等治理能力从应用中剥离，下沉到基础设施层。
-   **优点：** 非侵入性，多语言支持，统一管理，增强可观测性，高级流量管理。
-   **缺点：** 引入额外复杂性，性能开销，学习曲线。
-   **对比自实现：** 自实现是"库/SDK 模式"，适用于小规模、同构系统；Service Mesh 适用于大规模、异构、需要统一治理的系统。

## 4 网络

1. **网络基础层 (CNI)**: 负责为 Pod 插上"网线"，让 Pod 拥有 IP 地址，实现 Pod 之间的基础通信。
2. **服务发现与负载均衡层 (kube-proxy)**: 负责实现 Kubernetes `Service` 的概念，让你能够通过一个虚拟 IP 访问到后面的一组 Pod。
3. **应用层网络与服务治理层 (Service Mesh - Istio)**: 在应用层面提供更高级的流量控制、安全和可观察性。
4. **内核革新技术 (eBPF)**: 这是一种底层技术，可以用来"重塑"或"增强"以上各个层面的实现方式，带来巨大的性能和功能优势。

### 4.1 网络基础层：CNI (Container Network Interface)

首先，最基础的问题是：Pod 是如何获得 IP 地址并互相通信的？这就是 CNI 的职责。

CNI 是一个标准接口（一个规范），由 Cloud Native Computing Foundation (CNCF) 维护。它定义了容器运行时（如 Docker、containerd）和网络插件（CNI 插件）之间如何协作来配置容器网络。Kubelet 在创建和销毁 Pod 时，会调用安装在节点上的 CNI 插件。

1. **IP 地址管理 (IPAM)**: 为新创建的 Pod 分配一个 IP 地址。
2. **网络配置**: 将 Pod 连接到节点网络上。具体实现方式多样，比如创建 veth pair（虚拟网卡对）、配置网桥、设置路由规则等。

**Terway**: 这是阿里云开发的 CNI 插件。它的一个核心特点是**深度集成云厂商的 VPC（虚拟私有云）网络**。它可以直接为 Pod 分配 VPC 网络中的弹性网卡（ENI）和 IP 地址。

- **Terway 的优势**:
    1. **高性能**: Pod 的 IP 是真实的 VPC IP，流量路径更短，无需额外的封包解包（Overlay），网络性能接近物理机。
    2. **网络产品互通**: Pod 可以直接被 VPC 内的其他云产品（如数据库、缓存）访问，反之亦然，网络模型非常统一。
    3. **丰富的网络策略**: 支持与阿里云 VPC 安全组等产品联动。

**一句话总结 CNI**: CNI 是 Pod 网络的"水电工"，负责铺设基础管道，让数据可以在 Pod 之间最基础地流动起来。

### 4.2 服务发现与负载均衡层：kube-proxy

现在 Pod 之间可以通信了，但 Pod 的 IP 是会变化的。我们需要一个稳定的访问入口，这就是 `Service`。而 `kube-proxy` 就是 `Service` 的实现者。

kube-proxy 是运行在每个 Kubernetes 节点上的一个守护进程。它监视 API Server 中 Service 和 Endpoint（Service 后端 Pod 的 IP:Port 列表）对象的变化，并相应地修改节点上的网络规则。

**`iptables` 模式 (最常用，曾经的默认)**

- **原理**: 对于每一个 `Service`，`kube-proxy` 会创建一系列 `iptables` 规则。当流量访问 `Service` 的 ClusterIP 时，会经过这些规则链，最终通过 DNAT（目标地址转换）将流量重定向到后端某个具体的 Pod IP 上。负载均衡的逻辑（比如随机选择一个 Pod）也完全由 `iptables` 规则实现。
- **优点**: 成熟稳定，功能全面。
- **缺点**:
    - **性能瓶颈**: 当 Service 和 Pod 数量非常多时（比如上万个），iptables 规则会变得极其庞大和复杂，导致内核在处理每个数据包时都需要遍历很长的规则链，性能下降明显。
    - **调试困难**: `iptables` 规则的可读性非常差。

**`IPVS` 模式 (新一代默认)**

- **原理**: `IPVS`（IP Virtual Server）是 Linux 内核中专门用于负载均衡的模块，它使用哈希表来存储 Service 到 Pod 的映射关系。当流量访问 Service 的 ClusterIP 时，`IPVS` 能以近乎 O(1) 的时间复杂度快速找到目标 Pod 并转发。
- **优点**:
    - **高性能与高扩展性**: 即使 Service 数量巨大，也能保持高效的转发性能。
    - **更丰富的负载均衡算法**: 支持轮询、最少连接、加权等多种算法。
- **缺点**: `IPVS` 本身不处理 SNAT 等问题，仍需与 `iptables` 配合使用，但核心的负载均衡路径已经高效很多。

**一句话总结 kube-proxy**: `kube-proxy` 是 `Service` 的"交通警察"，通过修改 `iptables` 或 `IPVS` 规则，将访问虚拟 `Service IP` 的流量，指挥、分发到正确的后端 Pod 上。它工作在**网络层（L3）和传输层（L4）**。

### 4.3 应用层网络与服务治理层：Istio (Service Mesh)

`kube-proxy` 解决了 L4 的负载均衡，但如果我们想要更智能的控制，比如：A/B 测试（90% 流量到 v1，10% 到 v2）、请求超时重试、熔断、加密通信、详细的调用链监控等，`kube-proxy` 就无能为力了。这时 Istio 就登场了。

Istio 是一个服务网格（Service Mesh）平台。它通过在每个业务 Pod 中注入一个 Sidecar 代理（通常是 Envoy）来接管应用的所有出入流量。

1. **数据平面**: 所有流量都被 Sidecar 代理劫持。`Pod A` 发给 `Pod B` 的请求，实际路径是 `App A -> A的Sidecar -> B的Sidecar -> App B`。
2. **控制平面**: 你通过 Istio 的 CRD（如 `VirtualService`, `DestinationRule`）来定义流量规则。控制平面会将这些规则翻译成 Envoy 代理能理解的配置，并下发给所有 Sidecar。
3. **智能路由**: Sidecar 代理根据收到的配置，在**应用层（L7）** 对 HTTP/gRPC 等协议的流量进行精细化控制。例如，它可以检查 HTTP Header，根据其中的 `user-agent` 来决定是把请求发给 v1 版本还是 v2 版本的 Pod。我们上一轮讨论的基于`X-Session-ID`的**一致性哈希**就是在这里实现的。

**与kube-proxy的区别**

-  **OSI层级**: `kube-proxy`工作在L3/L4，只关心IP和端口。Istio工作在L7，能理解HTTP/gRPC等应用层协议。
- **感知能力**: `kube-proxy`只知道Service和Pod IP。Istio能感知到URL路径、请求头、gRPC方法等应用层细节。
- **功能**: `kube-proxy`做基础的负载均衡。Istio做智能路由、服务治理、安全和可观察性。
- **实现方式**: `kube-proxy`修改内核网络规则。Istio通过用户空间的Sidecar代理实现。

**一句话总结Istio**: Istio是应用的“智能网络助理”，通过Sidecar代理在应用层为服务间通信提供了强大的治理能力，但带来了额外的资源开销和延迟。

### 4.4 内核革新技术：eBPF

eBPF（extended Berkeley Packet Filter）是前面所有技术的"颠覆者"或"优化者"。

 eBPF 是一种革命性的内核技术，它允许你在 Linux 内核中运行一段沙箱化的、安全的自定义代码，而无需修改内核源码或加载内核模块。你可以把它想象成内核的"JavaScript"，当特定事件（如网络包到达、系统调用发生）触发时，执行你的代码。

eBPF 可以在网络数据路径的极早期（网卡驱动层）进行干预，从而用更高性能的方式重写 kube-proxy 和 CNI 的部分功能。

**与 kube-proxy 的区别**:

- 像 Cilium 这样的项目使用 eBPF 来完全替代 `iptables`。它在内核中创建一个高效的 eBPF Map（类似哈希表）来存储 Service到Pod的映射。当数据包进入节点时，eBPF程序会直接查找Map，完成地址转换并直接将包发往目标Pod的网络设备，绕过了复杂的`iptables`和`IPVS`路径。
- **优势**: 性能极高，路径极短，扩展性强。

**增强CNI**

- Cilium CNI也利用eBPF来实现Pod间通信和网络策略。它可以直接在内核层面执行网络策略，比`iptables`实现的网络策略效率高得多。

**替代Sidecar？**

- 最新的趋势是使用eBPF来实现**无Sidecar的服务网格（Sidecar-less Service Mesh）**。通过在每个节点上加载eBPF程序，来透明地实现部分Istio的功能（如路由、负载均衡、可观察性），从而避免了每个Pod都运行一个Sidecar代理的资源开销和网络延迟。

**与iptables/IPVS的区别**:

- **执行点**: `eBPF`在内核中更早的hook点执行，路径更短。
- **可编程性**: `iptables/IPVS`是配置式的，而`eBPF`是可编程的，提供了无限的可能性。
- **性能**: `eBPF`通常比`iptables`和`IPVS`有显著的性能优势。

**一句话总结eBPF**: eBPF是内核的“超能力插件”，可以用更原生、更高效的方式重构K8s的数据平面，实现CNI、kube-proxy甚至部分服务网格的功能。

### 4.5 总结对比



希望这个详细的分解能帮助你清晰地理解它们在 K8s 网络世界中各自的角色和区别！
