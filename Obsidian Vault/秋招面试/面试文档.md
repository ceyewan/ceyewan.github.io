---

date: 2025-09-14T20:33:55+08:00
draft: true
title: '未命名'
slug: '20250914-b4e62mdi'
tags:
  - 标签
categories:
  - 分类
---

## 1 自我介绍

面试官您好！我叫廖明秋，来自湖南衡阳，是武汉大学电子信息专业的在读硕士研究生，本科也同样毕业于武汉大学。非常荣幸能有机会参加今天的面试，感谢您的时间。

我主要专注于后端开发，熟练掌握 Go 语言，并具备 C++ 和 Python 的开发能力。在校期间，我对计算机基础知识进行了扎实学习，尤其在分布式系统和云原生领域有深入研究和实践，并积累了丰富的工程经验。我曾在阿里云容器服务部门实习，主要负责 ACK AI 助手的实现与带外数据链路建设。我还独立完成了一个微服务即时通讯项目，深入实践了 Etcd、gRPC、Kafka 等核心组件在微服务架构中的应用。

在个人特质方面，我热爱运动，长期坚持健身，培养了坚韧的毅力和高度的自律性。同时，我积极参与科研竞赛和学校活动，不断提升技术深度与团队协作能力，并乐于通过技术博客分享学习心得与项目经验。

我的优势在于对技术原理的深入探索能力、扎实的工程实践能力，以及强烈的自我驱动和责任感。非常期待能有机会加入贵公司，与优秀的团队共同成长，贡献我的力量。谢谢！

## 2 实习经历

面试官您好！我在阿里云容器服务部门实习期间，主要聚焦于两个核心方向：一是 ACK AI 助手的实现与工程化，二是对带外数据链路的建设。

首先，在 AI 助手方向，我的工作主要体现在三个层面：

1. ACK MCP Server 的实现与优化：
    - 我负责实现了 **ACK MCP Server**，其核心是将 **kubectl、OpenAPI SDK** 以及上下文补充等多种操作方法，统一封装为大模型（LLM）可稳定调用的**结构化 Tools**，并内建了完善的认证授权机制。
    - 为了显著降低模型的调用难度和减少幻觉，我对 API 描述与参数进行了优化，并内置了常用的 Prompt。这个成果最终开源，并提供了 Helm Chart 方便部署，实现了对外的服务能力。
2. Multi-Agent 架构的设计与实现：
    - 我基于 **LangGraph** 框架，设计并实现了一套 **“协调者-专家”分层式智能体架构**。
    - 我们通过 **ReAct（Reasoning and Acting）与 Plan-Execute 模式（并加入了 RePlan 机制）**，使得这套智能体系统既能够高效处理特定领域的复杂问题，也具备了通用任务的规划与执行能力。
3. AI 助手生产级工程化落地：
    - 为了确保 Agent 在生产环境的可用性与健壮性，我实现了多项关键模块，包括**结构化记忆、持久化会话管理、结合知识库的检索增强 (RAG)**，以及支持**人机协作 (HITL)** 的机制。
    - 同时，我也完成了对之前提到的 **MCP Server 的多租户改造**，确保了在多用户场景下的安全隔离与合规操作。

第二个方向是带外数据链路的建设：

- 这件事情的背景是传统的 Kubernetes 集群健康检查机制仅限于集群内部，无法感知来自底层基础设施（如物理硬件故障、宿主机计划性维护）的事件。
- **链路实现：** 我遵循 Kubernetes 的**控制器所有权模式**，构建了一条从底层基础设施到 ACK 集群的事件链路。具体是通过 SLS 订阅并解析来自 ECS 和灵骏等平台底层硬件的事件。
- 我设计了一个自动化流程，将这些原始故障 Event 转化为 Kubernetes 节点上的 **Label**。然后通过实时 **Watch Label 变化**，动态修改节点的 **Condition**，从而驱动 ACK 集群进行相应的自愈操作。
- 这个架构通过**事件驱动**保证了实时性，同时通过**定期同步**实现了最终一致性，极大地增强了系统的鲁棒性与可维护性。

我的最主要成果是成功交付了具备工具调用、知识检索、根因分析和自主修复能力的智能运维助手；同时，通过带外链路建设，我们将节点故障的感知时间从传统的 15 分钟大幅缩短至秒级，显著提升了 ACK 集群的自愈能力和用户体验。

## 3 即时通讯系统

面试官你好，第一个项目是我最近做的一个分布式的微服务即时通讯系统，支持高并发、低延迟和高可用性。

首先，这个系统采用了四层架构，分别是 API 接入层、逻辑处理层、任务调度层和长连接层，不同层次分别处理不同的内容，层级之间使用 etcd 来进行服务发现，使用 gRPC 和消息队列进行通信。

API 接入层是使用 GIN 框架和 JWT 构建的一个轻量级 Web 服务，为用户提供注册、登录、登出、权限验证等 HTTP 服务，接收到请求后，该层并不直接处理业务逻辑，而是将请求通过 RPC 调用转发给逻辑处理层。

逻辑处理层对外提供 RPC 服务，内部主要负责接收 API 接入层的用户登录、认证请求。此外，还需要出来来自长连接层的消息推送、状态变更。对于一条消息推送，逻辑处理层会将其处理后，为每一个消息通过分布式 ID 算法提交一个唯一的 ID，将其发送到消息队列中。此外，长连接层监控到有用户的上线和下线时，会将上线下线消息和对应的房间号发送到逻辑处理层，逻辑处理层使用 Redis 作为缓存，维护每一个房间的在线人数和成员信息，还会维护用户 ID 到长连接层实例 ID 的一个映射关系。

任务调度层当前分为两个消费者组从消息队列里获取消息，一个消费者组确保消息的及时投递，从消息队列里实时的消费消息后，根据消息是单聊消息还是群发消息，将其立即投递到对应的长连接层。第二个消费者组异步的从消息队列里读取消息，将消息内容持久化的写入到 MySQL 数据库中，确保消息的可靠性。

最后是长连接层，这一层直接面向用户，通过 Websocket 协议与用户建立持久的双向通信，当用户通过 API 接入层成功登录后，就会与长连接层连接 Websocket 连接。为每个 ws 连接分配两个协程，分别处理用户的发送消息和读取消息。用户发送的消息会被发送到逻辑处理层进行处理，从任务调度层获取到消息后，通过对应用户的 ws 长连接的写协程发送给用户。

为了提高系统的可拓展性和可靠性，采用了 etcd 作为服务发现中心，所有的服务实例均可实现动态扩缩容。最终达到了 5wQPS 的一个消息吞吐量和 50ms 的一个消息延时。

**下面我将以一条单聊消息的发送流程为例，串联起这四个层面：**

1. 用户在客户端发送一条消息，这条消息通过 HTTP 请求发送到 **API 接入层**。
2. **API 接入层** 接收到请求后，会调用 **LogicRPC** 服务（部署在 **逻辑处理层**），将消息内容、发送者和接收者信息传递给 **逻辑处理层**。
3. **逻辑处理层** 接收到消息后，首先进行必要的鉴权和业务逻辑处理，然后生成一个全局唯一的分布式消息 ID，并将该消息写入消息队列。同时，**逻辑处理层** 可能会通过查询 **Redis** 获取接收者的在线状态。
4. **任务调度层** 中的 **实时推送消费者组** 监听到新的消息后，会将其消费，并根据接收者的用户 ID，通过 **ConnectRPC** 调用相应的 **长连接层** 实例。
5. **长连接层** 接收到来自 **Task 层** 的推送请求后，会查找与接收者用户 ID 关联的 WebSocket 连接，并将消息通过该连接实时推送给用户的客户端。
6. 与此同时，**任务调度层** 中的 **消息持久化消费者组** 也会消费该消息，并将其异步地写入 **MySQL** 数据库进行持久化存储。

通过这样的分层设计和技术选型，我们的系统能够支持 **5W+ 的每秒查询率 (QPS)**，并保证消息投递延迟稳定在 **50 毫秒** 以内，同时具备秒级的故障恢复能力，能够高效地满足大规模用户的实时通讯需求。