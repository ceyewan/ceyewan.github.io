---
categories:
- Interview
date: 2025-09-21 14:13:48+08:00
draft: true
slug: 20250921-tvw7rz2k
summary: 本文详解游戏排行榜、高并发系统、流式计算与分布式任务调度设计，涵盖Redis分片、缓存优化、MapReduce处理及Gang调度等核心技术方案。
tags:
- System-Design
title: 系统设计面试题
---

## 1 设计游戏排行榜

问题：设计一个支持上亿用户的游戏排行榜，（绝大多数情况用户只会查看 Top-K 和自己的排名情况，可以估算。

### 1.1 数据存储和结构选择

- **Redis Sorted Sets**：适用于大规模排行榜，支持高效的插入、删除、查找 Top-K 排名等操作。
    
    - **插入/更新用户分数**：ZADD，O(log N)。
    - **查询 Top-K 排行**：ZREVRANGE，O(K)。
    - **查询用户排名**：ZREVRANK，O(log N)。

    Redis 的 Sorted Sets 非常适合处理大规模用户数据，能够在上亿用户的情况下保持高效性能。

### 1.2 数据分片

- **水平切分**：通过 **用户 ID** 或 **区服 ID** 来将排行榜数据分片，避免单个 Redis 实例的压力过大。

### 1.3 排行查询优化

- **Top-K 和用户排名查询优化**：
    - **缓存 Top-K 排行**：为了避免每次查询都直接访问 Redis，可以将 Top-K 排行缓存到内存或快速存储中，定期更新。
    - **缓存用户排名**：对于用户自己的排名，进行缓存，减少重复计算。

### 1.4 近似排名方案：桶排序

- **桶排序**：适合用来获取近似排名，尤其在分数范围较广的情况下，可以将用户分配到不同的桶中：
    - 根据用户的分数范围将数据分到不同的桶中（例如 0-1000、1001-2000 等），桶内再进行排序。
    - 通过查找桶的边界和桶内的排序，可以获得一个近似的排名，特别适合查询 Top-K 和用户大致排名。

### 1.5 高可用和容错

- **主从复制与哨兵机制**：设置 Redis 主从架构，并使用 Redis 哨兵机制来实现故障转移。
- **数据持久化**：定期使用 Redis 的 AOF 或 RDB 功能将数据持久化，确保在 Redis 崩溃时不会丢失重要数据。

## 2 设计一个高并发系统

设计一个高并发系统，特别是像 **购票系统** 这样的场景，涉及大量的并发请求、高性能处理以及数据一致性保证。我们可以从 **宏观（整体架构层面）** 和 **微观（细节优化层面）** 两个方向来考虑如何应对高并发。

### 2.1 宏观设计：整体架构层面

在宏观层面，主要考虑 **系统的可扩展性**、**高可用性**、**负载均衡** 和 **缓存设计** 等方面。

#### 2.1.1 分布式架构与微服务

- **微服务架构**：将购票系统拆分为多个微服务，每个微服务专注于特定的功能模块，如用户服务、票务服务、支付服务等。这样可以做到横向扩展，支持高并发。
- **服务拆分**：购票系统的不同功能可以拆分为多个服务，避免单点压力集中。例如，用户验证服务、票务查询服务、支付服务、订单管理服务等，彼此解耦，支持独立扩展。

#### 2.1.2 负载均衡与高可用性

- **DNS 负载均衡**：通过 **DNS** 将请求分配到多个可用区域，利用 DNS 解析的方式进行简单的流量分配。
- **负载均衡**：通过 **负载均衡器**（如 Nginx 或 LVS）将流量均匀分配到多个后端服务器或微服务实例中，避免单个节点的瓶颈。
- **高可用性**：采用 **主从复制** 和 **自动故障转移机制**（如使用 **Redis Sentinel** 或 **Kubernetes** 的健康检查与自动重启）来确保系统的高可用。

#### 2.1.3 缓存设计

缓存是高并发系统中优化性能和减少延迟的关键技术。

- **热点数据缓存**：使用 **Redis** 或 **Memcached** 缓存常用的查询结果，例如已售票数量、剩余票数等。
    - **分布式缓存**：通过 Redis 集群或分布式缓存来保证缓存的高可用性，防止单点故障。
    - **缓存穿透、缓存雪崩和缓存击穿问题**：通过合理的 **缓存过期时间** 和 **双重检查锁** 来解决这些问题。
    - **本地缓存**：针对一些高频访问的数据，可以使用本地缓存（如 **Go** 中的 **map** 或 **LRU 缓存**）来减少外部请求。

#### 2.1.4 异步处理与消息队列

- **消息队列**：使用 **Kafka**、**RabbitMQ** 等消息队列来解耦系统内部的各个模块，特别是涉及高并发的操作，比如 **订单确认** 和 **支付处理**。消息队列可以作为缓冲区，避免瞬时请求对系统造成过大的冲击。
- **异步处理**：对于一些非实时的操作（如发送短信通知、生成订单日志等），可以采用异步任务处理，减轻主流程的压力。

#### 2.1.5 静态资源缓存与 CDN

- **CDN（内容分发网络）**：将静态资源（如 **首页图片**、**广告**、**API 请求的结果**）通过 **CDN** 缓存到离用户最近的节点，减少从源服务器的请求压力。对于购票系统，这可以显著减轻来自静态资源（如票务列表、广告等）的访问压力。
    - **缓存静态内容**：如购票系统的静态页面、公告、促销信息等，可以通过 CDN 缓存来大幅减少数据库和后台服务的访问量。
    - **API 数据缓存**：例如对于频繁查询的热门票务数据、热门场次等，可以通过 CDN 和边缘节点的缓存来处理，减少中心服务器的压力。

### 2.2 微观设计：细节优化层面

在微观层面，考虑 **单个请求的高效处理** 和 **资源利用的最大化**。

#### 2.2.1 零拷贝技术

- **零拷贝（Zero-Copy）**：通过操作系统的零拷贝机制减少数据在内存中的复制，提高网络传输效率。比如，使用 **sendfile** 系统调用将数据直接从文件系统传输到网络连接中，避免了不必要的数据拷贝。
- **数据库读写优化**：通过 **数据库连接池** 和 **零拷贝技术**，减少数据库查询的延迟和资源占用，保证高并发情况下数据库的吞吐量。

#### 2.2.2 高效的数据库设计

- **数据库分片**：对数据库进行分片，根据不同的策略（如 **用户 ID**、**订单号**、**地理区域**）将数据分布到不同的数据库节点，避免单个数据库的负载过重。
- **读写分离**：通过 **主从复制** 实现读写分离，读操作可以通过从库进行，减轻主库的压力。
- **乐观锁与悲观锁**：通过 **乐观锁** 和 **事务隔离** 来保证数据一致性，避免因并发操作引发的数据冲突，确保购票操作的准确性。
- **合并请求（批处理）**：对于大量的并发请求，可以通过 **批处理** 和 **合并操作** 来减少数据库的访问次数，例如批量更新票务库存等。

#### 2.2.3 限流与熔断机制

- **限流**：在高并发场景下，可以使用 **令牌桶** 或 **漏桶算法** 来进行限流，控制请求的频率，防止系统过载。
- **熔断机制**：使用 **Hystrix** 或 **Sentinel** 等熔断框架，当系统的某个模块出现异常时，能够自动切换到降级策略，保护系统不被单一故障影响。

#### 2.2.4 连接池与资源复用

- **数据库连接池**：使用 **连接池** 来管理数据库连接，减少频繁创建和销毁连接带来的性能开销。
- **线程池与协程**：使用 **线程池** 或 **协程池**（如 **Go** 中的协程）来管理和复用工作线程，减少频繁创建和销毁线程的开销，提高并发处理能力。

#### 2.2.5 高效的网络 I/O

- **异步 I/O**：通过 **非阻塞 I/O** 和 **事件驱动模型**（如使用 **epoll**）来处理大量的并发连接，避免因为 I/O 操作阻塞导致的性能瓶颈。
- **HTTP/2 或 gRPC**：在 API 通信中使用 **HTTP/2** 或 **gRPC**，它们可以更高效地处理大量并发请求，支持多路复用，减少网络延迟。

## 3 设计一个流式计算系统

要设计一个 **全网投递广告系统**，跟踪广告点击次数并最终生成报告，我们可以采用 **低级的思路**，如 **MapReduce** 或传统的分布式计算方式。目的是让设计既简洁又高效，满足流量高峰时的处理需求，避免引入高级框架（如 Flink）。我们可以通过分布式系统设计来实现这个系统。

我们从 **数据采集**、**数据处理** 到 **报告生成** 分为几个阶段，采用 **MapReduce** 的思想来处理。

### 3.1 数据采集层

- **数据来源**：广告系统会产生一个 **点击事件流**，例如用户每次点击广告时，系统会记录用户信息（如用户 ID、广告 ID、时间戳等）。
- **数据存储**：所有点击数据首先通过 **消息队列**（如 Kafka 或 RabbitMQ）实时传输到后台系统，缓冲后再进行批量处理。

### 3.2 数据处理层：MapReduce

1. **Map 阶段**：
    - **输入数据**：点击日志（如 {user_id, ad_id, timestamp}）。
    - **Map 操作**：我们将每一条点击记录作为一个独立的 **map** 任务，将其映射为 (ad_id, 1)。即广告 ID 和点击次数为 1。
    - **分区**：为了高效处理，我们根据 **广告 ID** 对数据进行 **分区**。这样，相同广告 ID 的点击记录会被分到同一个分区进行处理。
2. **Shuffle 和 Sort 阶段**：
    - **Shuffle**：将来自不同 Map 节点的 (ad_id, 1) 对按 **广告 ID** 进行分组，确保每个广告的点击记录被收集到一起。
    - **Sort**：对每个广告的点击数据进行排序，可以选择按点击次数排序，或者保留原始的广告点击顺序，具体取决于业务需求。
3. **Reduce 阶段**：
    - **Reduce 操作**：对同一个广告的所有点击进行 **累加**，最终计算出每个广告的总点击次数。

例如，给定一个广告 ID，多个 (ad_id, 1) 会被归到同一个 Reduce 中，最终结果是广告 ID 和它的点击总数。

### 3.3 结果存储与报告生成

- **存储结果**：将最终的点击汇总结果存储到 **关系型数据库**（如 MySQL）、**NoSQL 数据库**（如 Cassandra）或 **文件系统**（如 HDFS、Amazon S3）中。可以存储格式为 {ad_id, total_clicks}，方便后续的分析和查询。
- **生成报告**：基于存储的结果，可以定时生成报告。报告可以是 **按广告 ID 排序的点击量**，或者按 **日期** 进行分组的点击统计。报告可以通过 SQL 查询、MapReduce 或编写自定义脚本来生成。

### 3.4 可扩展性与容错设计

- **水平扩展**：在流量高峰期间，我们可以通过增加 **Map** 和 **Reduce** 节点的数量来提高处理能力。使用分布式文件系统（如 HDFS）存储中间数据，并将任务分配到不同的机器节点上进行并行计算。
- **数据备份**：在消息队列和计算层，确保数据的 **持久化** 和 **备份**，以避免数据丢失。
- **容错性**：如果某个节点失败，其他节点可以继续工作，任务会在其他可用节点上重新执行。比如，使用 **任务重试** 和 **状态存储** 来保证最终结果的准确性。

## 4 设计一个分布式任务调度系统

一个健壮的分布式任务调度系统，其核心目标是**可靠、高效地执行海量任务，同时系统自身要具备高可用和高扩展性**。

我们可以把整个系统想象成一个高度协同的团队。这个团队主要有三个角色：

1. **调度中心 (大脑/指挥官):** 这是系统的核心决策者。它负责接收所有任务请求，并决定每个任务 " 何时 " 以及 " 由谁 " 来执行。为了防止这个 " 大脑 " 单点故障，通常会有多个调度中心实例待命，通过类似 ZooKeeper 这样的协调服务选举出一个 " 总指挥官 "（Leader），其他的则作为备份。如果现任指挥官意外下线，备份者会立刻接替，确保指挥系统永不中断。
2. **执行节点 (手脚/士兵):** 这是任务的实际执行者。系统中会有大量这样的节点，它们启动后会向 " 指挥官 " 报到，并定时汇报 " 我还活着 "（心跳机制），随时准备接受任务。这种设计的好处是，我们可以随时增减 " 士兵 " 数量来应对任务量的变化，实现弹性伸缩。
3. **任务队列 (中转站):** 这是连接 " 指挥官 " 和 " 士兵 " 的桥梁。指挥官决定好一个任务要执行后，并不会直接命令某个士兵去做，而是把任务指令书扔到这个中转站。士兵们会主动、有序地从中转站领取任务。这样做的好处是解耦，即使瞬间来了成千上万个任务，指挥官也能从容地处理，而士兵们则可以按照自己的节奏来执行，避免了系统被瞬时流量冲垮。

**工作流程**很简单：用户提交任务 -> 指挥官分析并下发到中转站 -> 某个士兵领取并执行 -> 将执行结果（成功/失败）汇报存档。如果一个士兵在执行任务时突然 " 牺牲 "（宕机），指挥官能通过心跳机制发现，并会将这个未完成的任务重新放回中转站，交由其他士兵完成，确保任务最终能被执行。

**现在，我们来考虑 " 组调度 "（Gang Scheduling）的特殊情况。**

常规任务是一个士兵就能完成的，但有些复杂任务，比如大规模数据处理或 AI 模型训练，需要一个"特战小队"（例如 10 个士兵）**必须同时开始、协同工作**才能完成。这就是组调度。

在这种模式下，"指挥官"的角色发生了关键变化，它从一个简单的任务 " 分发者 " 升级为 " 资源协调者 "。

其工作流程变为：

1. **资源预留:** 当指挥官收到一个需要 10 个士兵的 " 组任务 " 时，它不会立刻把任务扔到中转站。相反，它会开始清点当前所有空闲的士兵，并对满足条件的士兵进行 " 标记预留 "，确保它们不会被其他普通任务抢走。
2. **同步执行:** 指挥官会持续进行预留，直到凑齐所需的 10 个士兵。一旦集结完毕，它会同时向这 10 个士兵下达 " 立即行动 " 的指令，确保它们作为一个整体，在同一时刻启动任务。
3. **集体失败/成功:** 这个 " 特战小队 " 的命运是绑定的。在任务执行期间，只要有任何一个士兵失败或宕机，指挥官通常会中止整个小队的所有任务，然后尝试重新集结一个小队来重试，以保证任务的整体性和一致性。

通过这种 "**先预留、再同步**" 的机制，调度系统就能有效地处理需要多节点并行协作的复杂任务，满足了从简单脚本到复杂计算等多样化的业务需求。